{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9561839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import clickhouse_connect\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# START_SLOT = 370453701\n",
    "START_SLOT = 370656000  # Start of epoch 858\n",
    "END_SLOT = 377135999  # End of epoch 872\n",
    "TX_TYPES = [\"frontRun\", \"backRun\", \"victim\", \"transfer\"]\n",
    "ATTACKER_TX_TYPES = [\"frontRun\", \"backRun\", \"transfer\"]\n",
    "EPS_WIN = 1e-5\n",
    "\n",
    "\n",
    "def load_env():\n",
    "    load_dotenv(dotenv_path=\".env\")\n",
    "    return {\n",
    "        \"host\": os.getenv(\"NEW_CLICKHOUSE_HOST\"),\n",
    "        \"port\": int(os.getenv(\"NEW_CLICKHOUSE_PORT\")),\n",
    "        \"username\": os.getenv(\"NEW_CLICKHOUSE_USERNAME\"),\n",
    "        \"password\": os.getenv(\"NEW_CLICKHOUSE_PASSWORD\"),\n",
    "    }\n",
    "\n",
    "\n",
    "# Load credentials from .env\n",
    "config = load_env()\n",
    "# Initialize ClickHouse client\n",
    "client = clickhouse_connect.get_client(\n",
    "    host=config[\"host\"],\n",
    "    port=config[\"port\"],\n",
    "    username=config[\"username\"],\n",
    "    password=config[\"password\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d35cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_current_slots_in_DB(start_slot=0, end_slot=500000000):\n",
    "    \"\"\"\n",
    "    Query the ClickHouse DB to find the min and max slot stored\n",
    "    \"\"\"\n",
    "    if start_slot < START_SLOT:\n",
    "        print(\"Warning: start_slot is before the earliest valid slot in DB.\")\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        min(slot) AS min_slot,\n",
    "        max(slot) AS max_slot,\n",
    "        countDistinct(slot) AS total_slots\n",
    "    FROM solwich.slot_txs\n",
    "    WHERE slot BETWEEN {start_slot} AND {end_slot} AND txFetched = true\n",
    "    \"\"\"\n",
    "    result = client.query(query)\n",
    "    df = pd.DataFrame(result.result_rows, columns=result.column_names)\n",
    "    min_slot = df[\"min_slot\"].iloc[0]\n",
    "    max_slot = df[\"max_slot\"].iloc[0]\n",
    "    total_slots = df[\"total_slots\"].iloc[0]\n",
    "    return min_slot, max_slot, total_slots\n",
    "\n",
    "\n",
    "def query_leader_slot_counts(start_slot: int, end_slot: int) -> pd.DataFrame:\n",
    "    # Total slots led by each validator in the given slot range in schedule\n",
    "    if start_slot < START_SLOT:\n",
    "        print(\"Warning: start_slot is before the earliest valid slot in DB.\")\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        leader,\n",
    "        count() AS slot_count\n",
    "    FROM solwich.slot_leaders\n",
    "    WHERE slot BETWEEN {start_slot} AND {end_slot}\n",
    "    GROUP BY leader\n",
    "    ORDER BY slot_count DESC\n",
    "    \"\"\"\n",
    "    res_schedule = client.query(query)\n",
    "    df_schedule = pd.DataFrame(\n",
    "        res_schedule.result_rows, columns=res_schedule.column_names\n",
    "    )\n",
    "\n",
    "    # Total slots actually led by each validator in the given slot range, checked in DB\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        l.leader,\n",
    "        countDistinct(s.slot) AS actual_slot_count\n",
    "    FROM solwich.slot_leaders AS l\n",
    "    LEFT JOIN solwich.slot_txs  AS s\n",
    "        ON s.slot = l.slot\n",
    "    WHERE l.slot BETWEEN {start_slot} AND {end_slot} AND txFetched = true\n",
    "    GROUP BY l.leader\n",
    "    \"\"\"\n",
    "    res_actual = client.query(query)\n",
    "    df_actual = pd.DataFrame(res_actual.result_rows, columns=res_actual.column_names)\n",
    "\n",
    "    out = df_schedule.merge(df_actual, on=\"leader\", how=\"left\")\n",
    "    out[\"actual_slot_count\"] = out[\"actual_slot_count\"].fillna(0).astype(int)\n",
    "    out[\"slot_count\"] = out[\"slot_count\"].astype(int)\n",
    "    out = (\n",
    "        out.rename(columns={\"leader\": \"validator\"})\n",
    "        .sort_values([\"actual_slot_count\", \"slot_count\"], ascending=[False, False])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def query_sandwiches_group_by_leader(start_slot: int, end_slot: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Count sandwiches data per validator in the given slot range.\n",
    "    \"\"\"\n",
    "    if start_slot < START_SLOT:\n",
    "        print(\"Warning: start_slot is before the earliest valid slot in DB.\")\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        l.leader,\n",
    "        countDistinct(s.sandwichId) AS sandwich_count,               \n",
    "        sum(s.victimCount) AS total_victim_count,      \n",
    "        sumIf(s.profitA, s.tokenA = 'SOL') AS total_SOL_profit\n",
    "    FROM solwich.sandwiches AS s\n",
    "    INNER JOIN solwich.slot_leaders AS l ON s.slot = l.slot\n",
    "    WHERE s.slot BETWEEN {start_slot} AND {end_slot}\n",
    "    GROUP BY l.leader\n",
    "    ORDER BY total_SOL_profit DESC\n",
    "    \"\"\"\n",
    "    result = client.query(query)\n",
    "    res = pd.DataFrame(result.result_rows, columns=result.column_names)\n",
    "    res.rename(columns={\"leader\": \"validator\"}, inplace=True)\n",
    "    return res\n",
    "\n",
    "\n",
    "def query_sandwiches_with_txs_and_leader(\n",
    "    start_slot: int, end_slot: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Step 1:\n",
    "    - From solwich.sandwiches, fetch all sandwiches in [start_slot, end_slot].\n",
    "    - Ensure each sandwichId appears at most once using LIMIT 1 BY sandwichId.\n",
    "    Step 2:\n",
    "    - Fetch all sandwiches' txs (*) from solwich.sandwich_txs.\n",
    "    - Deduplicate txs where (type, signature) are the same\n",
    "    - Return the integrated rows: sandwich columns (*) + tx columns (*).\n",
    "    Step 3:\n",
    "    - Join slot_leaders on each tx's slot to add leader for every row.\n",
    "    \"\"\"\n",
    "    if start_slot < START_SLOT:\n",
    "        print(\"Warning: start_slot is before the earliest valid slot in DB.\")\n",
    "    query = f\"\"\"\n",
    "    WITH dedup_sandwiches AS\n",
    "    (\n",
    "        SELECT\n",
    "            sandwichId, slot as sandwich_slot, crossBlock, tokenA, tokenB, signerSame, ownerSame, profitA, relativeDiffB\n",
    "        FROM solwich.sandwiches\n",
    "        WHERE slot BETWEEN {start_slot} AND {end_slot}\n",
    "        ORDER BY slot DESC, sandwichId DESC\n",
    "        LIMIT 1 BY sandwichId\n",
    "    ),\n",
    "    dedup_txs AS\n",
    "    (\n",
    "        SELECT\n",
    "            *\n",
    "        FROM solwich.sandwich_txs\n",
    "        WHERE sandwichId IN (SELECT sandwichId FROM dedup_sandwiches) AND slot BETWEEN {start_slot} AND {end_slot}\n",
    "        ORDER BY slot DESC\n",
    "        LIMIT 1 BY sandwichId, type, signature\n",
    "    ),\n",
    "    s_leaders AS \n",
    "    (\n",
    "        SELECT\n",
    "            *\n",
    "        FROM solwich.slot_leaders\n",
    "        WHERE slot BETWEEN {start_slot} AND {end_slot}\n",
    "        ORDER BY slot DESC\n",
    "    )\n",
    "    SELECT\n",
    "        s.sandwichId as sandwichId,\n",
    "        s.sandwich_slot,\n",
    "        l.leader,\n",
    "        s.crossBlock,\n",
    "        s.tokenA,\n",
    "        s.tokenB,\n",
    "        s.signerSame,\n",
    "        s.ownerSame,\n",
    "        s.profitA,\n",
    "        s.relativeDiffB,\n",
    "        t.type,\n",
    "        t.slot as tx_slot,\n",
    "        t.position,\n",
    "        t.timestamp,\n",
    "        t.fee,\n",
    "        t.signature,\n",
    "        t.signer,\n",
    "        t.inBundle,\n",
    "        t.programs,\n",
    "        t.fromToken,\n",
    "        t.toToken,\n",
    "        t.fromAmount,\n",
    "        t.toAmount,\n",
    "        t.attackerPreBalanceB,\n",
    "        t.attackerPostBalanceB,\n",
    "        t.ownersOfB,\n",
    "        t.fromTotalAmount,\n",
    "        t.toTotalAmount,\n",
    "        t.diffA,\n",
    "        t.diffB\n",
    "    FROM dedup_sandwiches AS s\n",
    "    INNER JOIN dedup_txs AS t\n",
    "        ON t.sandwichId = s.sandwichId\n",
    "    LEFT JOIN s_leaders AS l\n",
    "        ON l.slot = t.slot\n",
    "    ORDER BY sandwich_slot DESC\n",
    "    \"\"\"\n",
    "    result = client.query(query)\n",
    "    res = pd.DataFrame(result.result_rows, columns=result.column_names)\n",
    "    res.rename(columns={\"leader\": \"validator\"}, inplace=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34bd8240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fetched slots in DB: min = 370656000, min_epoch = 858, max = 377135999, max_epoch = 872, total = 6447648, fetch proportion = 0.9950\n"
     ]
    }
   ],
   "source": [
    "min_slot, max_slot, total_slots = query_current_slots_in_DB(\n",
    "    start_slot=START_SLOT, end_slot=END_SLOT\n",
    ")\n",
    "min_epoch = min_slot // 432000\n",
    "max_epoch = max_slot // 432000\n",
    "print(\n",
    "    f\"Current fetched slots in DB: min = {min_slot}, min_epoch = {min_epoch}, max = {max_slot}, max_epoch = {max_epoch}, total = {total_slots}, fetch proportion = {total_slots / (END_SLOT - START_SLOT + 1):.4f}\"\n",
    ")\n",
    "\n",
    "# prev_epoch, prev_epoch_start, prev_epoch_end = fetch_prev_epoch_info()\n",
    "# print(f\"Previous epoch: {prev_epoch}, start: {prev_epoch_start}, end: {prev_epoch_end}\")\n",
    "\n",
    "# min_slot, max_slot, total_slots = query_current_slots_in_DB(\n",
    "#     prev_epoch_start, prev_epoch_end\n",
    "# )\n",
    "# print(\n",
    "#     f\"Current fetched slots in DB in previous epoch range: min = {min_slot}, max = {max_slot}, total = {total_slots}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0c0c9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total slots led in epoch 858 to 872 (slots 370656000 to 377135999):\n",
      "                                      validator  slot_count  actual_slot_count\n",
      "0  HEL1USMZKAL2odpNBj2oCjffnFGaYwmbGmyewGv1e2TU      213940             213080\n",
      "1  DRpbCBMxVnDK7maPM5tGv6MvB3v1sRMC86PZ8okm21hy      193168             192511\n",
      "2   JupmVLmA8RoyTUbTMMuTtoPWHEiNQobxgTeGTrPNkzT      176676             175933\n",
      "3  Fd7btgySsrjuo25CJCj7oE7VPMyezDhnx7pZkj2v69Nk      170748             169967\n",
      "4  DtdSSG8ZJRZVv5Jx7K1MeWp7Zxcu19GD5wQRGRpQ9uMF      134880             134131\n",
      "5  5pPRHniefFjkiaArbGX3Y8NUysJmQ9tMZg3FrFGwHzSm      130404             129918\n",
      "6   q9XWcZ7T1wP4bW9SB4XgNNwjnFEJ982nE8aVbbNuwot      129640             129239\n",
      "7  JD549HsbJHeEKKUrKgg4Fj2iyv2RGjsV7NTZjZUrHybB      104892             104466\n",
      "8  Awes4Tr6TX8JDzEhCZY2QVNimT6iD1zWHzf1vNyGvpLM      105084             104357\n",
      "9  EvnRmnMrd69kFdbLMxWkTn1icZ7DCceRhvmb2SJXqDo4       97104              96697\n",
      "Total slots: 6480000 Total actual slots checked: 6447648\n",
      "                                      validator  sandwich_count  \\\n",
      "0   JupmVLmA8RoyTUbTMMuTtoPWHEiNQobxgTeGTrPNkzT           71784   \n",
      "1  DRpbCBMxVnDK7maPM5tGv6MvB3v1sRMC86PZ8okm21hy           79595   \n",
      "2  5pPRHniefFjkiaArbGX3Y8NUysJmQ9tMZg3FrFGwHzSm           68656   \n",
      "3  BfgMdL4FaNHp5zZpD7WMYG5sZUrCWQPEjXDwWS7M5q3F           17318   \n",
      "4  HEL1USMZKAL2odpNBj2oCjffnFGaYwmbGmyewGv1e2TU           87717   \n",
      "5  Awes4Tr6TX8JDzEhCZY2QVNimT6iD1zWHzf1vNyGvpLM           53511   \n",
      "6  DtdSSG8ZJRZVv5Jx7K1MeWp7Zxcu19GD5wQRGRpQ9uMF           58030   \n",
      "7  JD549HsbJHeEKKUrKgg4Fj2iyv2RGjsV7NTZjZUrHybB           47931   \n",
      "8  Fd7btgySsrjuo25CJCj7oE7VPMyezDhnx7pZkj2v69Nk           72965   \n",
      "9  9jxgosAfHgHzwnxsHw4RAZYaLVokMbnYtmiZBreynGFP           49606   \n",
      "\n",
      "   total_victim_count  total_SOL_profit  \n",
      "0              385270        210.581205  \n",
      "1              445101        192.576324  \n",
      "2              366377        172.505120  \n",
      "3               39057        157.694737  \n",
      "4              435389        153.372084  \n",
      "5              283743        141.653355  \n",
      "6              239527        140.677606  \n",
      "7              210456        136.656700  \n",
      "8              401604        116.333862  \n",
      "9              226884        113.063669  \n",
      "Total sandwiches: 2771401\n",
      "Total profit in SOL: 6331.842902845392\n",
      "Total victims: 13837128\n",
      "Leader statistic with victims/slot, sandwiches/slot, profit/slot:\n",
      "                                        validator  slot_count  \\\n",
      "932   Rosss8KdLc366Hg8tqigieoLiXWUwguw9giX4HP1UsE          52   \n",
      "931  6dpdFgXyTGFTQkefKNTx6qgqwGEQa9GE1msghJTZoxQJ          56   \n",
      "960   KAW1LjxH73tRBd1XsaqsRsgeERFkg4WpdXUSqR4QjkW          12   \n",
      "956   KAoSp3EudGqUBXv46tQoDwbZxSm3iXa9wM2aF4ySbJJ          16   \n",
      "941  BULKzVM41WAyQZfL34vxqdsYwEYH9mJAJyzRS4xraf8b          32   \n",
      "100  BfgMdL4FaNHp5zZpD7WMYG5sZUrCWQPEjXDwWS7M5q3F       12772   \n",
      "246  EXckihF3qmguH5znjhfzLvHsbk2E3nEW2DqNh4MMnDMm        3968   \n",
      "895  DRFh448Zz6AaHLJPZTgSw4fwv3eu776Db5RYEY4BqT7X         140   \n",
      "845   fdzip81euDS8jEZHx5H1mn27zGVMLzkgpQuzYRBfBYG         328   \n",
      "921  GQqxGEmi6aMBZtcfmfmC5Jgx33X57ksvNYoo8bMH52T9          76   \n",
      "\n",
      "     actual_slot_count  sandwich_count  total_victim_count  total_SOL_profit  \\\n",
      "932                 48           120.0               715.0         -0.023824   \n",
      "931                 52            97.0               567.0         -0.113245   \n",
      "960                 12            22.0                70.0          0.000161   \n",
      "956                 16            23.0                46.0         -0.067788   \n",
      "941                 32            44.0               165.0          0.017948   \n",
      "100              12691         17318.0             39057.0        157.694737   \n",
      "246               3931          5199.0             26657.0         10.954391   \n",
      "895                132           171.0               453.0          0.215807   \n",
      "845                324           395.0              2530.0          0.060600   \n",
      "921                 76            88.0               343.0          0.055831   \n",
      "\n",
      "     victims_per_slot  sandwiches_per_slot  profit_per_slot  \n",
      "932         14.895833             2.500000        -0.000496  \n",
      "931         10.903846             1.865385        -0.002178  \n",
      "960          5.833333             1.833333         0.000013  \n",
      "956          2.875000             1.437500        -0.004237  \n",
      "941          5.156250             1.375000         0.000561  \n",
      "100          3.077535             1.364589         0.012426  \n",
      "246          6.781226             1.322564         0.002787  \n",
      "895          3.431818             1.295455         0.001635  \n",
      "845          7.808642             1.219136         0.000187  \n",
      "921          4.513158             1.157895         0.000735  \n"
     ]
    }
   ],
   "source": [
    "leader_slots = query_leader_slot_counts(min_slot, max_slot)\n",
    "print(\n",
    "    f\"Total slots led in epoch {min_epoch} to {max_epoch} (slots {min_slot} to {max_slot}):\"\n",
    ")\n",
    "print(leader_slots.head(10))\n",
    "print(\n",
    "    \"Total slots:\",\n",
    "    leader_slots[\"slot_count\"].sum(),\n",
    "    \"Total actual slots checked:\",\n",
    "    leader_slots[\"actual_slot_count\"].sum(),\n",
    ")\n",
    "\n",
    "# Query victims number, sandwich number and SOL profit per validator in the previous epoch\n",
    "leader_sandwiches = query_sandwiches_group_by_leader(min_slot, max_slot)\n",
    "print(leader_sandwiches.head(10))\n",
    "print(\"Total sandwiches:\", leader_sandwiches[\"sandwich_count\"].sum())\n",
    "print(\"Total profit in SOL:\", leader_sandwiches[\"total_SOL_profit\"].sum())\n",
    "print(\"Total victims:\", leader_sandwiches[\"total_victim_count\"].sum())\n",
    "# Compute victims per slot, sandwiches per slot, profit per slot\n",
    "leader_statistic = leader_slots.merge(leader_sandwiches, on=\"validator\", how=\"left\")\n",
    "leader_statistic[\"victims_per_slot\"] = leader_statistic.apply(\n",
    "    lambda row: (\n",
    "        row[\"total_victim_count\"] / row[\"actual_slot_count\"]\n",
    "        if row[\"actual_slot_count\"] > 0\n",
    "        else 0\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "leader_statistic[\"sandwiches_per_slot\"] = leader_statistic.apply(\n",
    "    lambda row: (\n",
    "        row[\"sandwich_count\"] / row[\"actual_slot_count\"]\n",
    "        if row[\"actual_slot_count\"] > 0\n",
    "        else 0\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "leader_statistic[\"profit_per_slot\"] = leader_statistic.apply(\n",
    "    lambda row: (\n",
    "        row[\"total_SOL_profit\"] / row[\"actual_slot_count\"]\n",
    "        if row[\"actual_slot_count\"] > 0\n",
    "        else 0\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "leader_statistic = leader_statistic.sort_values(\n",
    "    by=[\"sandwiches_per_slot\", \"profit_per_slot\", \"victims_per_slot\"],\n",
    "    ascending=False,\n",
    ")\n",
    "print(\"Leader statistic with victims/slot, sandwiches/slot, profit/slot:\")\n",
    "print(leader_statistic.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c97e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m end = max_slot\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# start = START_SLOT\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# end = 380000000\u001b[39;00m\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Slots with sandwiches and their txs\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m sandwiches_txs = \u001b[43mquery_sandwiches_with_txs_and_leader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     10\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal rows of sandwiches with txs and leader in slots from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: sandwiches - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mleader_sandwiches[\u001b[33m'\u001b[39m\u001b[33msandwich_count\u001b[39m\u001b[33m'\u001b[39m].sum()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sandwich_txs - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sandwiches_txs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Profit (now in SOL only)\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# profit_view = sandwiches_txs.drop_duplicates(subset=[\"sandwichId\"]).copy()\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# profit_view = profit_view[[\"sandwichId\", \"tokenA\", \"profitA\"]]\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# profit_view[\"is_SOL\"] = profit_view[\"tokenA\"] == \"SOL\"\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 173\u001b[39m, in \u001b[36mquery_sandwiches_with_txs_and_leader\u001b[39m\u001b[34m(start_slot, end_slot)\u001b[39m\n\u001b[32m    107\u001b[39m query = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[33mWITH dedup_sandwiches AS\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[33m(\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    170\u001b[39m \u001b[33mORDER BY sandwich_slot DESC\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    172\u001b[39m result = client.query(query)\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m res = pd.DataFrame(\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult_rows\u001b[49m, columns=result.column_names)\n\u001b[32m    174\u001b[39m res.rename(columns={\u001b[33m\"\u001b[39m\u001b[33mleader\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mvalidator\u001b[39m\u001b[33m\"\u001b[39m}, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/clickhouse_connect/driver/query.py:290\u001b[39m, in \u001b[36mQueryResult.result_rows\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    288\u001b[39m result = []\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.row_block_stream \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[38;5;28mself\u001b[39m._result_rows = result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/clickhouse_connect/driver/common.py:208\u001b[39m, in \u001b[36mStreamContext.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_context:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ProgrammingError(\u001b[33m'\u001b[39m\u001b[33mStream should be used within a context\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/clickhouse_connect/driver/query.py:310\u001b[39m, in \u001b[36mQueryResult._row_block_stream\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_row_block_stream\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_column_block_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/clickhouse_connect/driver/transform.py:72\u001b[39m, in \u001b[36mNativeTransform.parse_response.<locals>.gen\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m first_block\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     next_block = \u001b[43mget_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m next_block \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     74\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/clickhouse_connect/driver/transform.py:52\u001b[39m, in \u001b[36mNativeTransform.parse_response.<locals>.get_block\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     50\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     51\u001b[39m             context.start_column(orig_name)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m             column = \u001b[43mcol_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m             result_block.append(column)\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/clickhouse_connect/datatypes/base.py:154\u001b[39m, in \u001b[36mClickHouseType.read_column\u001b[39m\u001b[34m(self, source, num_rows, ctx)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03mWrapping read method for all ClickHouseType data types.  Only overridden for container classes so that\u001b[39;00m\n\u001b[32m    147\u001b[39m \u001b[33;03m the LowCardinality version is read for the contained types\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    151\u001b[39m \u001b[33;03m:return: The decoded column data as a sequence\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    153\u001b[39m read_state = \u001b[38;5;28mself\u001b[39m.read_column_prefix(source, ctx)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_column_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/clickhouse_connect/datatypes/base.py:170\u001b[39m, in \u001b[36mClickHouseType.read_column_data\u001b[39m\u001b[34m(self, source, num_rows, ctx, read_state)\u001b[39m\n\u001b[32m    168\u001b[39m     column = \u001b[38;5;28mself\u001b[39m._read_nullable_column(source, num_rows, ctx, read_state)\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     column = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_column_binary\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._finalize_column(column, ctx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/clickhouse_connect/datatypes/string.py:34\u001b[39m, in \u001b[36mString._read_column_binary\u001b[39m\u001b[34m(self, source, num_rows, ctx, _read_state)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_read_column_binary\u001b[39m(\u001b[38;5;28mself\u001b[39m, source: ByteSource, num_rows: \u001b[38;5;28mint\u001b[39m, ctx: QueryContext, _read_state: Any):\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msource\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_str_col\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_active_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/clickhouse_connect/driver/buffer.py:100\u001b[39m, in \u001b[36mResponseBuffer.read_str_col\u001b[39m\u001b[34m(self, num_rows, encoding, nullable, null_obj)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m encoding:\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m         \u001b[43mapp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[32m    102\u001b[39m         app(x.hex())\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "start = min_slot\n",
    "end = max_slot\n",
    "\n",
    "# start = START_SLOT\n",
    "# end = 380000000\n",
    "\n",
    "# Slots with sandwiches and their txs\n",
    "sandwiches_txs = query_sandwiches_with_txs_and_leader(start, end)\n",
    "print(\n",
    "    f\"Total rows of sandwiches with txs and leader in slots from {start} to {end}: sandwiches - {leader_sandwiches['sandwich_count'].sum()} sandwich_txs - {len(sandwiches_txs)}\"\n",
    ")\n",
    "\n",
    "# Profit (now in SOL only)\n",
    "profit_view = sandwiches_txs.drop_duplicates(subset=[\"sandwichId\"]).copy()\n",
    "profit_view = profit_view[[\"sandwichId\", \"tokenA\", \"profitA\"]]\n",
    "profit_view[\"profit_SOL\"] = profit_view[\"profitA\"].where(\n",
    "    profit_view[\"tokenA\"] == \"SOL\", 0.0\n",
    ")\n",
    "profit_view[\"is_SOL\"] = profit_view[\"tokenA\"] == \"SOL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb80de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bundle_status(row_tx_count, row_inbundle_count):\n",
    "    if row_tx_count == 0:\n",
    "        return \"no_tx\"\n",
    "    if row_inbundle_count == 0:\n",
    "        return \"none_in_bundle\"\n",
    "    if row_inbundle_count == row_tx_count:\n",
    "        return \"all_in_bundle\"\n",
    "    return \"partial_in_bundle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0c8f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Jito bundle coverage analysis\n",
    "# jito_view = sandwiches_txs.copy()\n",
    "# jito_stat = jito_view.groupby(\"sandwichId\", as_index=False).agg(\n",
    "#     tx_count=(\"signature\", \"count\"),\n",
    "#     inbundle_count=(\"inBundle\", lambda x: x.fillna(False).astype(int).sum()),\n",
    "# )\n",
    "# jito_stat = jito_stat.merge(profit_view, on=\"sandwichId\", how=\"left\")\n",
    "# jito_stat[\"tokenA\"] = jito_stat[\"tokenA\"].fillna(\"UNKNOWN\")\n",
    "# jito_stat[\"profitA\"] = jito_stat[\"profitA\"].fillna(0.0)\n",
    "\n",
    "# jito_stat[\"bundle_status\"] = jito_stat.apply(\n",
    "#     lambda row: bundle_status(row[\"tx_count\"], row[\"inbundle_count\"]), axis=1\n",
    "# )\n",
    "# summary = (\n",
    "#     jito_stat.groupby(\"bundle_status\", dropna=False)\n",
    "#     .apply(\n",
    "#         lambda r: pd.Series(\n",
    "#             {\n",
    "#                 \"sandwich_count\": r[\"sandwichId\"].nunique(),\n",
    "#                 \"total_profit_SOL\": r.loc[r[\"tokenA\"] == \"SOL\", \"profitA\"].sum(),\n",
    "#                 \"sol_sandwich_count\": (r[\"tokenA\"] == \"SOL\").sum(),\n",
    "#                 \"avg_profit_SOL\": r.loc[r[\"tokenA\"] == \"SOL\", \"profitA\"].mean(),\n",
    "#             }\n",
    "#         )\n",
    "#     )\n",
    "#     .reset_index()\n",
    "# )\n",
    "# total_with_tx = int(summary[\"sandwich_count\"].sum())\n",
    "# summary[\"sandwich_count_share\"] = (\n",
    "#     summary[\"sandwich_count\"] / total_with_tx if total_with_tx > 0 else 0.0\n",
    "# )\n",
    "# print(\n",
    "#     f\"Jito bundle coverage (slots {start} to {end}): total sandwiches = {total_with_tx}\"\n",
    "# )\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31a087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance analysis\n",
    "distance_view = sandwiches_txs.copy()\n",
    "\n",
    "dist_rows = []\n",
    "for sid, txs in distance_view.groupby(\"sandwichId\"):\n",
    "    cross_block = bool(txs[\"crossBlock\"].iloc[0])\n",
    "    tx_count = int(len(txs))\n",
    "    inbundle_count = int(txs[\"inBundle\"].sum())\n",
    "    status = bundle_status(tx_count, inbundle_count)\n",
    "\n",
    "    # Consecutive\n",
    "    pos_unique = sorted(txs[\"position\"].dropna().unique().tolist())\n",
    "    consecutive = False\n",
    "    if not cross_block and len(pos_unique) > 0:\n",
    "        consecutive = pos_unique[-1] - pos_unique[0] + 1 == len(pos_unique)\n",
    "\n",
    "    # Distance\n",
    "    fr = txs[txs[\"type\"] == \"frontRun\"]\n",
    "    br = txs[txs[\"type\"] == \"backRun\"]\n",
    "    last_front_pos = fr[\"position\"].max() if not fr.empty else None\n",
    "    first_back_pos = br[\"position\"].min() if not br.empty else None\n",
    "    last_front_slot = fr[\"tx_slot\"].max() if not fr.empty else None\n",
    "    first_back_slot = br[\"tx_slot\"].min() if not br.empty else None\n",
    "\n",
    "    inblock_distance = None\n",
    "    if not cross_block and last_front_pos is not None and first_back_pos is not None:\n",
    "        inblock_distance = int(first_back_pos - last_front_pos)\n",
    "\n",
    "    crossblock_gap_slots = None\n",
    "    if cross_block and (last_front_slot is not None) and (first_back_slot is not None):\n",
    "        crossblock_gap_slots = int(first_back_slot - last_front_slot)\n",
    "\n",
    "    dist_rows.append(\n",
    "        {\n",
    "            \"sandwichId\": sid,\n",
    "            \"cross_block\": cross_block,\n",
    "            \"consecutive\": bool(consecutive),\n",
    "            \"bundle_status\": status,  # all/partial/none/no_tx\n",
    "            \"tx_count\": tx_count,\n",
    "            \"inbundle_count\": inbundle_count,\n",
    "            \"inblock_distance\": inblock_distance,\n",
    "            \"crossblock_gap_slots\": crossblock_gap_slots,\n",
    "        }\n",
    "    )\n",
    "\n",
    "dist_stat = pd.DataFrame(dist_rows)\n",
    "dist_stat = dist_stat.merge(profit_view, on=\"sandwichId\", how=\"left\")\n",
    "dist_stat[\"tokenA\"] = dist_stat[\"tokenA\"].fillna(\"UNKNOWN\")\n",
    "dist_stat[\"profitA\"] = dist_stat[\"profitA\"].fillna(0.0)\n",
    "\n",
    "# -------- In block (consecutive) --------\n",
    "inblock_consecutive = dist_stat[\n",
    "    (~dist_stat[\"cross_block\"]) & (dist_stat[\"consecutive\"])\n",
    "]\n",
    "print(\"\\n[] In-block & consecutive sandwiches\")\n",
    "print(f\"Count: {len(inblock_consecutive)}\")\n",
    "cov1 = inblock_consecutive.groupby(\"bundle_status\").apply(\n",
    "    lambda r: pd.Series(\n",
    "        {\n",
    "            \"count\": len(r),\n",
    "            \"total_profit_SOL\": r.loc[r[\"tokenA\"] == \"SOL\", \"profitA\"].sum(),\n",
    "            \"sol_sandwich_count\": (r[\"tokenA\"] == \"SOL\").sum(),\n",
    "            \"avg_profit_SOL\": r.loc[r[\"tokenA\"] == \"SOL\", \"profitA\"].mean(),\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "total1 = int(cov1[\"count\"].sum()) if not cov1.empty else 0\n",
    "cov1[\"share\"] = cov1[\"count\"] / total1\n",
    "print(\"Jito bundle coverage (all/partial/none):\")\n",
    "print(cov1)\n",
    "\n",
    "# -------- In block (non-contiguous) --------\n",
    "inblock_non_contig = dist_stat[\n",
    "    (~dist_stat[\"cross_block\"]) & (~dist_stat[\"consecutive\"])\n",
    "]\n",
    "print(\"\\n[2] In-block & non-contiguous sandwiches\")\n",
    "print(f\"Count: {len(inblock_non_contig)}\")\n",
    "dist = inblock_non_contig[\"inblock_distance\"].dropna().astype(int)\n",
    "print(\"Distance summary (first_backrun.position - last_frontrun.position):\")\n",
    "print(dist.describe())\n",
    "print(\"Top distances (value counts):\")\n",
    "print(dist.value_counts().head(10))\n",
    "cov2 = inblock_non_contig.groupby(\"bundle_status\").apply(\n",
    "    lambda r: pd.Series(\n",
    "        {\n",
    "            \"count\": len(r),\n",
    "            \"total_profit_SOL\": r.loc[r[\"tokenA\"] == \"SOL\", \"profitA\"].sum(),\n",
    "            \"sol_sandwich_count\": (r[\"tokenA\"] == \"SOL\").sum(),\n",
    "            \"avg_profit_SOL\": r.loc[r[\"tokenA\"] == \"SOL\", \"profitA\"].mean(),\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "total2 = int(cov2[\"count\"].sum()) if not cov2.empty else 0\n",
    "cov2[\"share\"] = cov2[\"count\"] / total2\n",
    "print(\"Jito bundle coverage (all/partial/none):\")\n",
    "print(cov2)\n",
    "\n",
    "# -------- Cross block --------\n",
    "cross_block_df = dist_stat[dist_stat[\"cross_block\"]]\n",
    "print(\"\\n[3] Cross-block sandwiches\")\n",
    "print(f\"Count: {len(cross_block_df)}\")\n",
    "gaps = cross_block_df[\"crossblock_gap_slots\"].dropna().astype(int)\n",
    "print(\"Gap slots summary (min backrun slot - max frontrun slot):\")\n",
    "print(gaps.describe())\n",
    "print(\"Top gap slots (value counts):\")\n",
    "print(gaps.value_counts().head(10))\n",
    "cov3 = cross_block_df.groupby(\"bundle_status\").apply(\n",
    "    lambda r: pd.Series(\n",
    "        {\n",
    "            \"count\": len(r),\n",
    "            \"total_profit_SOL\": r.loc[r[\"tokenA\"] == \"SOL\", \"profitA\"].sum(),\n",
    "            \"sol_sandwich_count\": (r[\"tokenA\"] == \"SOL\").sum(),\n",
    "            \"avg_profit_SOL\": r.loc[r[\"tokenA\"] == \"SOL\", \"profitA\"].mean(),\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "total3 = int(cov3[\"count\"].sum()) if not cov3.empty else 0\n",
    "cov3[\"share\"] = cov3[\"count\"] / total3\n",
    "print(\"Jito bundle coverage (all/partial/none):\")\n",
    "print(cov3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69837a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnionFind:\n",
    "    def __init__(self):\n",
    "        self.parent = {}\n",
    "        self.rank = {}\n",
    "\n",
    "    def find(self, x):\n",
    "        self.parent.setdefault(x, x)\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "\n",
    "    def union(self, a, b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra == rb:\n",
    "            return\n",
    "        self.rank.setdefault(ra, 0)\n",
    "        self.rank.setdefault(rb, 0)\n",
    "        if self.rank[ra] < self.rank[rb]:\n",
    "            self.parent[ra] = rb\n",
    "        elif self.rank[ra] > self.rank[rb]:\n",
    "            self.parent[rb] = ra\n",
    "        else:\n",
    "            self.parent[rb] = ra\n",
    "            self.rank[ra] += 1\n",
    "\n",
    "\n",
    "# Attacker analysis\n",
    "uf = UnionFind()\n",
    "\n",
    "attacker_view = sandwiches_txs.copy()\n",
    "attacker_profit_view = profit_view.copy().set_index(\"sandwichId\")\n",
    "\n",
    "# Link every signer to an attacker using union-find\n",
    "for sid, txs in attacker_view[attacker_view[\"type\"].isin(ATTACKER_TX_TYPES)].groupby(\n",
    "    \"sandwichId\"\n",
    "):\n",
    "    attacker_txs = txs[txs[\"type\"].isin(ATTACKER_TX_TYPES)]\n",
    "    attacker_signers = sorted(set(attacker_txs[\"signer\"].dropna().astype(str).tolist()))\n",
    "\n",
    "    signer_same = (\n",
    "        bool(txs[\"signerSame\"].dropna().iloc[0])\n",
    "        if not txs[\"signerSame\"].empty\n",
    "        else False\n",
    "    )\n",
    "    if signer_same and len(attacker_signers) != 1:\n",
    "        print(\n",
    "            f\"Warning: sandwichId {sid} has signerSame={signer_same} but multiple signers: {attacker_signers}\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    if len(attacker_signers) <= 1:\n",
    "        if len(attacker_signers) == 1:\n",
    "            _ = uf.find(attacker_signers[0])\n",
    "        continue\n",
    "    else:\n",
    "        base = attacker_signers[0]\n",
    "        for other in attacker_signers[1:]:\n",
    "            uf.union(base, other)\n",
    "\n",
    "comp_members = defaultdict(set)\n",
    "for s in sorted(set(attacker_view[\"signer\"].dropna().astype(str))):\n",
    "    comp_members[uf.find(s)].add(s)\n",
    "\n",
    "# Map root to attacker key (hashing sorted concatenation of members)\n",
    "root_to_attacker_key = {\n",
    "    root: hex(abs(hash(\",\".join(sorted(members)))))\n",
    "    for root, members in comp_members.items()\n",
    "}\n",
    "signer_to_attacker_key = {\n",
    "    s: root_to_attacker_key[uf.find(s)]\n",
    "    for s in set(attacker_view[\"signer\"].dropna().astype(str))\n",
    "}\n",
    "\n",
    "\n",
    "# Collect attacker's sandwich data\n",
    "def choose_attacker_for_sandwich(df):\n",
    "    ss = df[df[\"type\"].isin(ATTACKER_TX_TYPES)][\"signer\"].dropna().astype(str).tolist()\n",
    "    keys = sorted(\n",
    "        {signer_to_attacker_key.get(s) for s in ss if s in signer_to_attacker_key}\n",
    "    )\n",
    "    # Only one attacker\n",
    "    if len(keys) == 0:\n",
    "        return \"UNKNOWN\"\n",
    "    if len(keys) > 1:\n",
    "        print(\n",
    "            f\"Warning: sandwichId {df['sandwichId'].iloc[0]} has multiple attackers: {keys}\"\n",
    "        )\n",
    "    return keys[0] if len(keys) == 1 else \" + \".join(sorted(set(keys)))\n",
    "\n",
    "\n",
    "print(attacker_view.columns.tolist())\n",
    "sandwich_to_attacker = (\n",
    "    attacker_view.groupby(\"sandwichId\", as_index=False)\n",
    "    .apply(choose_attacker_for_sandwich)\n",
    "    .rename(columns={None: \"attacker_key\"})\n",
    ")\n",
    "\n",
    "attacker_stat = sandwich_to_attacker.merge(\n",
    "    profit_view, on=\"sandwichId\", how=\"left\"\n",
    ").merge(jito_stat[[\"sandwichId\", \"bundle_status\"]], on=\"sandwichId\", how=\"left\")\n",
    "attacker_stat[\"profit_SOL\"] = attacker_stat[\"profit_SOL\"].fillna(0.0)\n",
    "attacker_stat[\"is_SOL\"] = attacker_stat[\"is_SOL\"].fillna(False)\n",
    "attacker_stat[\"win\"] = (attacker_stat[\"profit_SOL\"] > 1e-5).astype(int)\n",
    "\n",
    "attacker_summary = (\n",
    "    attacker_stat.groupby(\"attacker_key\", dropna=False)\n",
    "    .apply(\n",
    "        lambda r: pd.Series(\n",
    "            {\n",
    "                \"sandwich_count\": r[\"sandwichId\"].nunique(),\n",
    "                \"total_profit_SOL\": r.loc[r[\"is_SOL\"], \"profit_SOL\"].sum(),\n",
    "                \"sol_sandwich_count\": r[\"is_SOL\"].sum(),\n",
    "                \"avg_profit_SOL\": r.loc[r[\"is_SOL\"], \"profit_SOL\"].mean(),\n",
    "                \"win_count\": r[\"win\"].sum(),\n",
    "                \"win_rate\": r[\"win\"].mean(),\n",
    "                \"all_in_bundle\": (r[\"bundle_status\"] == \"all_in_bundle\").sum()\n",
    "                / r[\"sandwichId\"].nunique(),\n",
    "                \"partial_in_bundle\": (r[\"bundle_status\"] == \"partial_in_bundle\").sum()\n",
    "                / r[\"sandwichId\"].nunique(),\n",
    "                \"none_in_bundle\": (r[\"bundle_status\"] == \"none_in_bundle\").sum()\n",
    "                / r[\"sandwichId\"].nunique(),\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "attacker_addresses = (\n",
    "    pd.Series(\n",
    "        {\n",
    "            root_to_attacker_key[root]: sorted(list(members))\n",
    "            for root, members in comp_members.items()\n",
    "        },\n",
    "        name=\"signer_addresses\",\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"attacker_key\"})\n",
    ")\n",
    "attacker_addresses[\"signer_address_count\"] = attacker_addresses[\n",
    "    \"signer_addresses\"\n",
    "].apply(len)\n",
    "attacker_addresses[\"signer_addresses_str\"] = attacker_addresses[\n",
    "    \"signer_addresses\"\n",
    "].apply(lambda xs: \", \".join(xs))\n",
    "attacker_summary = attacker_summary.merge(\n",
    "    attacker_addresses, on=\"attacker_key\", how=\"left\"\n",
    ").sort_values(by=[\"win_count\", \"win_rate\"], ascending=False)\n",
    "\n",
    "print(\"\\n[Attacker Entities] summary (top 20):\")\n",
    "print(attacker_summary.head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
